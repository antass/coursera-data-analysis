[Data Analysis] Week 6 Lectures
================================================================================

**Preflight:**
```{r preflight}
setwd("~/Desktop/coursera-data-analysis/lectures/")
```

----

Prediction Study Design
--------------------------------------------------------------------------------

[slides](https://dl.dropbox.com/u/7710864/courseraPublic/week6/001predictionStudyDesign/index.html) |
[video](https://class.coursera.org/dataanalysis-001/lecture/download.mp4?lecture_id=123) |
[transcript](https://class.coursera.org/dataanalysis-001/lecture/subtitles?q=123_en&format=txt)

"How to design and evaluate predictive functions."

**Key ideas**
- Motivation
  - why would you do prediction?
- Steps in predictive studies
- Choosing the right data
  - even if you have "a bunch of data", you won't necessarily be able to make strong predctions
- Error measures
- Study design
  - careful not to fool yourself (re: that you made an accurate/strong prediction)

**Why predict?**

- glory! fame!
- money! riches!
- just for fun!
- to save lives!

### Steps in building a prediction
1. Find the right data
   - (do you even have data that you can use for preduction?)
   - are the indicators obvious? or are they hard to trust?
     - know when to quit (don't keep trying if it looks like you can't actually
       make good predictions from the data you have)
   - what actually makes a "good prediction"? (what does "good prediction" mean?)
   - **"know the benchmark"** - what are you trying to beat?
     - Probability of perfect classification is approximately: $(\frac{1}{2})^{test\ set\ sample\ size}$
     - key point: big enough test set to distinguish your model's predictions from "just chance"
2. Define your error rate
   - **critical!** (too often skipped)
   - typically used for binary classification: ![Define your error rate](https://dl.dropbox.com/u/7710864/courseraPublic/week6/001predictionStudyDesign/assets/img/sensspec.png)
   - **False Positive** = (Type I error)
   - **False Negative** = (Type II error)
   - important: **Sensitivity** and **Specificity** can help define avg. quality of a particular test
   - getting more specific w/ that framework: ![Why your choice matters](https://dl.dropbox.com/u/7710864/courseraPublic/week6/001predictionStudyDesign/assets/img/sensspecex.png)
     - pay attn: good-looking Sensitivity and great-looking Specificity scores _but..._
     - Positive predictive value is _quite low_ (10%)
   - some other common error measures:
     - Mean squared error (or root mean squared error)  
       better for continuous values (sensitive to outliers)
     - Median absolute deviation  
       also better for continuous values (more robust than mean squared error)
     - sensitivity (recall) & specificity
     - accuracy  
       for binary data  
       weights false positives & negatives equally
     - concordance  
       for multiple predictors  
       how well do they coordinate in making predictions
3. Split data into:
   - Training
   - Testing
   - Validation (optional)
   - _remember:_ don't _overfit_ your data
4. On the training set pick features
5. On the training set pick prediction function
6. On the training set cross-validate
7. If no validation - apply 1x to test set
8. If validation - apply to test set and refine
9. If validation - apply 1x to validation

### Defining true/false positives
(Typically applied for binary outcomes)

- generally: **Positive** = identified and **Negative** = rejected
- **True positive** = correctly identified
  - it _is_ interesting, and your model identifed it as interesting
- **False positive** = incorrectly identified
  - it _is not_ interesting, and your model identifed it as interesting
- **True negative** = correctly rejected
  - it _is not_ interesting, and your model identifed it as _not_ interesting
- **False negative** = incorrectly rejected
  - it _is_ interesting, and your model identifed it as _not_ interesting

### Study design
- _All_ data
  - split into _training_ set and _hold out_ set
- training set: available to everyone
- hold out set: not available to people building predictive models
  - _probe_ - models built against training set could be run against probe to test veracity
  - _quiz_ - like probe, but reserved for model validators
  - _test_ - for final analysis of predictive models



----

Cross-Validation
--------------------------------------------------------------------------------

[slides](https://dl.dropbox.com/u/7710864/courseraPublic/week6/002crossValidation/index.html) |
[video](https://class.coursera.org/dataanalysis-001/lecture/download.mp4?lecture_id=125) |
[transcript](https://class.coursera.org/dataanalysis-001/lecture/subtitles?q=125_en&format=txt)

**Key ideas**
- Sub-sampling the training data
  - focus on the training sample - then sub-sample even further
- Avoiding overfitting
  - reminder: do _not_ tune your model too strongly toward your sample data
- Making predictions generalizable

### Steps in building a prediction

4. On the training set **pick features**
5. On the training set **pick prediction function**
6. On the training set **cross-validate**

**GOAL** of _cross-validation_: estimate how well your predictive function will work on the "test" data

### Overfitting
```{r}
set.seed(12345)
x <- rnorm(10); y <- rnorm(10)
z <- rbinom(10,size=1,prob=0.5)
plot(x,y,pch=19,col=(z+3))
```

#### Classifier
If $-0.2 < y < 0.6$ call blue, otherwise green

```{r}
par(mfrow=c(1,2))
zhat <- (-0.2 < y) & (y < 0.6)
plot(x,y,pch=19,col=(z+3))
plot(x,y,pch=19,col=(zhat+3))
```

"Model is perfect! even though green and blue are unrelated" (i.e., you really _shouldn't_ be able to build a model)

#### New data
If $-0.2 < y < 0.6$ call blue, otherwise green

```{r}
set.seed(1233)
xnew <- rnorm(10)
ynew <- rnorm(10)
znew <- rbinom(10,size=1,prob=0.5)
par(mfrow=c(1,2))
zhatnew <- (-0.2 < ynew) & (ynew < 0.6)
plot(xnew,ynew,pch=19,col=(z+3)); plot(xnew,ynew,pch=19,col=(zhatnew+3))
```

_See:_ The model makes bad predictions, b/c it was (effectively) assuming/illustrating a relationship that wasn't really there. (It was "too tuned" or _over-fitted_.)

**Key ideas**
1. Accuracy on the training set (resubstitution accuracy) is _optimistic_
2. A better estimate comes from an _independent set_ (test set accuracy)
3. But we can't use the test set when building the model or it becomes part of the training set
4. So we estimate the test set accuracy with the training set.

### Cross-validation
_Approach:_
1. Use the training set
2. Split it into training/test sets
3. Build a model on the training set
4. Evaluate on the test set
5. Repeat and average the estimated errors

_Used for:_
1. Picking variables to include in a model
   - (**feature selection**)
2. Picking the type of prediction function to use
3. Picking the parameters in the prediction function
4. Comparing different predictors

#### Approaches/Methods
- random sub-sampling: ![Random subsampling](https://dl.dropbox.com/u/7710864/courseraPublic/week6/002crossValidation/assets/img/random.png)
  - take random "stripes" of data & split: training & test samples
  - advantages: balance size of training & test sets
  - disadvantages: possible to have elements that are repeated across sets
- K-fold: ![K-fold](https://dl.dropbox.com/u/7710864/courseraPublic/week6/002crossValidation/assets/img/kfold.png)
  - example (above) = "3-fold..."
  - take a chunk & withhold as testing; train on the rest
    - repeat w/ next (third); repeat w/ next third...
  - extreme example of K-fold: "leave one out sampling": ![Leave one out sampling](https://dl.dropbox.com/u/7710864/courseraPublic/week6/002crossValidation/assets/img/loocv.png)
  - "LEAVE ONE OUT" = less biased, but less stable
  - _Ed. note: "bias" is a fancy way of saying over-fitting?_

#### Example
```{r}
y1 <- y[1:5]
x1 <- x[1:5]
z1 <- z[1:5]
y2 <- y[6:10]
x2 <- x[6:10]
z2 <- z[6:10]
zhat2 <- (y2 < 1) & (y2 > -0.5)
par(mfrow=c(1,3))
plot(x1,y1,col=(z1+3),pch=19)
plot(x2,y2,col=(z2+3),pch=19)
plot(x2,y2,col=(zhat2+3),pch=19)
```
(Should help us to avoid being "overly optimistic")



----

Predicting with Regression
--------------------------------------------------------------------------------

[slides](https://dl.dropbox.com/u/7710864/courseraPublic/week6/003predictingRegression/index.html) |
[video](https://class.coursera.org/dataanalysis-001/lecture/download.mp4?lecture_id=119) |
[transcript](https://class.coursera.org/dataanalysis-001/lecture/subtitles?q=119_en&format=txt)

**Key ideas**
- Use a standard regression model
  - `lm`
  - `glm`
- Predict new values with the coefficients
- Useful when the linear model is (nearly) correct
- Pros:
  - Easy to implement
  - Easy to interpret
  - (relatively) easy to get measures of error (confidence intervals, etc.)
- Cons:
  - Often poor performance in nonlinear settings

### Case Study: Old Faithful Eruptions
```{r}
data(faithful)
dim(faithful)

## splitting the set into a training set:
set.seed(333)
trainSamples <- sample(1:272,size=(272/2),replace=F)
trainFaith <- faithful[trainSamples,]
testFaith <- faithful[-trainSamples,]
head(trainFaith)

# graph it:
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
# note the clusters?
```

#### Fit a linear model
$$ ED_i = b_0 + b_1 WT_i + e_i $$
```{r}
lm1 <- lm(eruptions ~ waiting,data=trainFaith)
summary(lm1)

### Model fit
plot(trainFaith$waiting, trainFaith$eruptions,
     pch=19, col="blue", xlab="Waiting", ylab="Duration")
lines(trainFaith$waiting, lm1$fitted, lwd=3)
```

#### Predict a new value
"That's exactly how it works..."

$$ \hat{ED} = \hat{b}_0 + \hat{b}_1 WT $$

- predicted value (of eruption duration, $\hat{ED}$) equal to output of the model; so...
- estimated intercept plus ($\hat{b}_0$)
- estimated slope times ($\hat{b}_1$)
- the coefficient (wait time, $WT$)
```{r}
# calculated it manually:
coef(lm1)[1] + coef(lm1)[2]*80

# or use R's `predict` to do the math for you
newdata <- data.frame(waiting=80)
predict(lm1, newdata)

### Plot predictions - training and test
par(mfrow=c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions,
     pch=19, col="blue", xlab="Waiting", ylab="Duration")
lines(trainFaith$waiting, predict(lm1), lwd=3)
plot(testFaith$waiting, testFaith$eruptions,
     pch=19, col="blue", xlab="Waiting", ylab="Duration")
lines(testFaith$waiting, predict(lm1,newdata=testFaith), lwd=3)

### Get training set/test set errors
# Calculate root mean squared error (RMSE) on training
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
# Calculate RMSE on test
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))
# note how the RMSE is slightly higher when calculated from the test set (vs. the training set)
# that's to be expected (for obvious reasons)

### Prediction intervals
pred1 <- predict(lm1, newdata=testFaith, interval="prediction")
ord <- order(testFaith$waiting)
par(mfrow=c(1,1))
plot(testFaith$waiting, testFaith$eruptions,
     pch=19, col="blue")
matlines(testFaith$waiting[ord], pred1[ord,], type="l", ,
         col=c(1,2,2), lty = c(1,1,1), lwd=3)
```

### Case Study: (Binary Data) B'more Ravens
Wins & Losses in the 2012-2013 season...
```{r}
download.file("https://dl.dropbox.com/u/7710864/data/ravensData.rda",
              destfile="./data/ravensData.rda", method="curl")
load("./data/ravensData.rda")
head(ravensData)
```

#### Fit a logistic regression
$$ logit(E[RW_i|RS_i]) = b_0 + b_1 RS_i $$
```{r}
glm1 <- glm(ravenWinNum ~ ravenScore,
            family="binomial", data=ravensData)
par(mfrow=c(1,2))
boxplot(predict(glm1) ~ ravensData$ravenWinNum, col="blue")
boxplot(predict(glm1,type="response") ~ ravensData$ravenWinNum, col="blue")

# output of that regression model is the probability that the Ravens will win...

### Choosing a cutoff (re-substitution)
xx <- seq(0,1,length=10)
err <- rep(NA,10)
# loop through each cut-off and plot the number of errors -- where was your lowest number of errors? that's your best cut-off value!
for(i in 1:length(xx)) {
  err[i] <- sum((predict(glm1, type="response") > xx[i]) != ravensData$ravenWinNum)
}
plot(xx, err, pch=19, xlab="Cutoff", ylab="Error")

### Comparing models with cross validation
library(boot)
cost <- function(win, pred = 0) mean(abs(win-pred) > 0.5)
glm1 <- glm(ravenWinNum ~ ravenScore, family="binomial", data=ravensData)
glm2 <- glm(ravenWinNum ~ ravenScore, family="gaussian", data=ravensData)
cv1 <- cv.glm(ravensData,glm1,cost,K=3)
cv2 <- cv.glm(ravensData,glm2,cost,K=3)

cv1$delta

cv2$delta
```

(slide 17: notes & further reading)



----

Predicting with Trees
--------------------------------------------------------------------------------

[slides](https://dl.dropbox.com/u/7710864/courseraPublic/week6/004predictingTrees/index.html) |
[video](https://class.coursera.org/dataanalysis-001/lecture/download.mp4?lecture_id=121) |
[transcript](https://class.coursera.org/dataanalysis-001/lecture/subtitles?q=121_en&format=txt)

> "Trees allow you to predict with functions that can capture non-linearities
> much more easily than linear models can."

**Key ideas**
- Iteratively split variables into groups
- Split where maximally predictive
- Evaluate "homogeneity" within each branch
- Fitting multiple trees often works better (forests)
- Pros:
  - Easy to implement
  - Easy to interpret
  - Better performance in nonlinear settings
- Cons:
  - Without pruning/cross-validation can lead to overfitting
  - Harder to estimate uncertainty
  - Results may be variable

### Example Tree

### Basic algorithm
1. Start with all variables in one group
2. Find the variable/split that best separates the outcomes
3. Divide the data into two groups ("leaves") on that split ("node")
4. Within each split, find the best variable/split that separates the outcomes
5. Continue until the groups are too small or sufficiently "pure"

### Measures of impurity
- MATH! ([slide 5](https://dl.dropbox.com/u/7710864/courseraPublic/week6/004predictingTrees/index.html#5))
- Misclassification error
- Gini index
- Cross-entropy or deviance

### Example: Iris Data
```{r}
data(iris)
names(iris)

table(iris$Species)

### Iris petal widths/sepal width
plot(iris$Petal.Width, iris$Sepal.Width,
     pch=19, col=as.numeric(iris$Species))
legend(1, 4.5, legend=unique(iris$Species),
       col=unique(as.numeric(iris$Species)), pch=19)

# An alternative is library(rpart)
library(tree)
# `tree` (looks a bit like `lm`, eh?)
tree1 <- tree(Species ~ Sepal.Width + Petal.Width, data=iris)
summary(tree1)

### Plot tree
plot(tree1)
text(tree1)

### Another way of looking at a CART model
plot(iris$Petal.Width, iris$Sepal.Width,
     pch=19, col=as.numeric(iris$Species))
partition.tree(tree1, label="Species", add=TRUE)
legend(1.75, 4.5, legend=unique(iris$Species),
       col=unique(as.numeric(iris$Species)), pch=19)

### Predicting new values
set.seed(32313)
newdata <- data.frame(Petal.Width = runif(20,0,2.5), Sepal.Width = runif(20,2,4.5))
pred1 <- predict(tree1,newdata)
pred1

### Overlaying new values
pred1 <- predict(tree1,newdata,type="class")
plot(newdata$Petal.Width,newdata$Sepal.Width,col=as.numeric(pred1),pch=19)
partition.tree(tree1,"Species",add=TRUE)
```

### Pruning trees example: Cars
```{r}
data(Cars93, package="MASS")
head(Cars93)

### Build a tree
treeCars <- tree(DriveTrain ~ MPG.city + MPG.highway + AirBags + 
                   EngineSize + Width + Length + Weight + Price + Cylinders + 
                   Horsepower + Wheelbase,data=Cars93)
plot(treeCars)
text(treeCars)

### Plot errors
# `cv.tree` for 'cross-validate tree'
par(mfrow=c(1,2))
plot(cv.tree(treeCars,FUN=prune.tree,method="misclass"))
plot(cv.tree(treeCars))

### Prune the tree
par(mfrow=c(1,1))
pruneTree <- prune.tree(treeCars,best=4)
plot(pruneTree)
title(main="Pruning the Tree...")
text(pruneTree)

### Show resubstitution error
table(Cars93$DriveTrain,predict(pruneTree,type="class"))
table(Cars93$DriveTrain,predict(treeCars,type="class"))
```