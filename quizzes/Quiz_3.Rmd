# Week 3 Quiz

## Question 1
Below is a plot of bone density versus age. It was created using the following
code in R:

```{r}
library(ElemStatLearn)
data(bone)
plot(bone$age,bone$spnbmd,pch=19,col=((bone$gender=="male")+1))
```

Males are shown in black and females in red. What are the characteristics that
make this an exploratory graph? Check all correct options.

+ **There plot does not have a legend.**
+ **The axis labels are R variables**
- The plot uses filled in circles.
- The plot is made in R.
- The plot uses color to make the figure "pretty"
+ **The plot does not report the units on the axis labels.**

## Question 2
Below is a boxplot of yearly income by marital status for individuals in the
United States. It was created using the following code in R:

```{r}
library(ElemStatLearn)
data(marketing)
plot(bone$age,bone$spnbmd,pch=19,col=((bone$gender=="male")+1))
boxplot(marketing$Income ~ marketing$Marital,col="grey",xaxt="n",ylab="Income",xlab="")
axis(side=1,at=1:5,labels=c("Married","Living together/not married","Divorced or separated","Widowed","Nevermarried"),las=2)
```

Which of the following can you conclude from the plot? (Check all that apply)

+ **There are more individuals who were never married than divorced in this data set.**
- The 75th percentile of the income for individuals who were widowed is almost the same as the 75th percentile for individuals who were married.
- There are more individuals who are widowed than divorced in this data set.
+ **The median income for individuals who are divorced is higher than the median for individuals who are widowed.**
+ **The 75th percentile of the income for widowed individuals is almost the same as the 75th percentile for never married individuals..**

## Question 3
Load the iris data into R using the following commands:

```{r}
library(datasets)
data(iris)
```

Subset the `iris` data to the first four columns and call this matrix
`irisSubset`. Apply hierarchical clustering to the irisSubset data frame to
cluster the rows. If I cut the dendrogram at a height of 3 how many clusters
result?

```{r}
## SHOW YOUR WORK:
irisSubset <- iris[,1:4]
plot(hclust(dist(irisSubset)))

## Brendan suggests:
rect.hclust(hclust(dist(irisSubset)), h=3)
```

**4 clusters**

## Question 4

Load the following data set into R using either the .rda or .csv file: 

* https://spark-public.s3.amazonaws.com/dataanalysis/quiz3question4.rda 
* https://spark-public.s3.amazonaws.com/dataanalysis/quiz3question4.csv 

Make a scatterplot of the `x` versus `y` values. How many clusters do you
observe? Perform k-means clustering using your estimate as to the number of
clusters. Color the scatterplot of the `x`, `y` values by what cluster they
appear in. Is there anything wrong with the resulting cluster estimates?

```{r}
## SHOW YOUR WORK:
setwd("~/Desktop/coursera-data-analysis/quizzes")
download.file('https://spark-public.s3.amazonaws.com/dataanalysis/quiz3question4.csv',
              "quiz3question4.csv", method="curl")
q3q4.data <- read.csv("quiz3question4.csv", header=TRUE)
q3q4.scatter <- plot(q3q4.data$x, q3q4.data$y)

# How many clusters do you observe?
# ~~4?~~ **2**

# Perform k-means clustering...
kmeansObj <- kmeans(q3q4.data, centers=2)
plot(q3q4.data$x, q3q4.data$y, col=kmeansObj$cluster, pch=19, cex=2)
```

~~There are two obvious clusters. The k-means algorithm correctly assigns all points to the two obvious clusters.~~
**There are two obvious clusters. The k-means algorithm does not assign all of the points to the correct clusters because the clusters wrap around each other.**

## Question 5
Load the hand-written digits data using the following commands:

```{r}
library(ElemStatLearn)
data(zip.train)
```

Each row of the `zip.train` data set corresponds to a hand written digit. The
first column of the zip.train data is the actual digit. The next 256 columns
are the intensity values for an image of the digit. To visualize the digit we
can use the `zip2image()` function to convert a row into a 16 x 16 matrix:

```{r}
# Create an image matrix for the 3rd row, which is a 4
im = zip2image(zip.train,3)
image(im)
```

Using the `zip2image` file, create an image matrix for the 8th and 18th rows.
For each image matrix calculate the `svd` of the matrix (with no scaling). What
is the percent variance explained by the first singular vector for the image
from the 8th row? What is the percent variance explained for the image from the
18th row? Why is the percent variance lower for the image from the 18th row?

```{r}
## SHOW YOUR WORK:
im8 <- zip2image(zip.train, 8)
im18 <- zip2image(zip.train, 18)

svd8 <- svd(im8)
svd18 <- svd(im18)

par(mfrow=c(2,2))
plot(svd8$d^2/sum(svd8$d^2),xlab="Column",ylab="Percent of variance explained",pch=19)
plot(svd18$d^2/sum(svd18$d^2),xlab="Column",ylab="Percent of variance explained",pch=19)
image(im8)
image(im18)
```

~~The first singular vector explains 98% of the variance for row 8 and 48% for row 18. The reason the first singular vector explains less variance for the 18th row is because the 8th row has higher average values.~~
**The first singular vector explains 98% of the variance for row 8 and 48% for row 18. The reason the first singular vector explains less variance for the 18th row is that the image is more complicated, so there are multiple patterns each explaining a large percentage of variance.**