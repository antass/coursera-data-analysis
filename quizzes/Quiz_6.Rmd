# Week 6 Quiz

## Question 1

Which of the following (pick one) is _not_ a step in building a prediction model?

1st: **Estimating test set accuracy with training-set accuracy.**  
2nd: **Selecting features with the test set.**

## Question 2

If K is small in a K-fold cross validation is the bias in the estimate of
out-of-sample (test set) accuracy smaller or bigger? If K is small is the
variance in the estimate of out-of-sample (test set) accuracy smaller or bigger.
Is K large or small in leave one out cross validation?

**The bias is larger and the variance is smaller. Under leave one out cross validation K is equal to the sample size.**

## Question 3

Load the South Africa Heart Disease Data and create training and test sets with
the following code:

```{r q3data}
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
```

Then fit a logistic regression model with Coronary Heart Disease (chd) as the
outcome and age at onset, current alcohol consumption, obesity levels,
cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as
predictors. Calculate the misclassification rate for your model using this
function and a prediction on the "response" scale:

```{r q3missClassFn}
missClass = function(values,prediction) {
  sum(((prediction > 0.5)*1) != values) / length(values)
}
```

What is the misclassification rate on the training set? What is the
misclassification rate on the test set?

```{r q3showYourWork}
### ????
glm1 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family="binomial", data=trainSA)

# this isn't what you're supposed to do ... and it doesn't work anyway but still...
#glm2 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family="binomial", data=testSA)

missClass(trainSA$chd, predict(glm1, type="response"))
# NOT : missClass(testSA$chd, predict(glm1, type="response"))
# Dave points out to pass it `newdata`
missClass(testSA$chd, predict(glm1, newdata=testSA, type="response"))
```

**Training set misclassification: 0.2727 / Test set misclassification: 0.3117**  
_AND/BUT that wasn't what I got for the test set misclassification value..._

## Question 4

Load the olive oil data using the commands:

```{r q4data}
library(pgmm)
data(olive)
olive = olive[,-1]
```

These data contain information on 572 different Italian olive oils from multiple
regions in Italy. Fit a classification tree where Area is the outcome variable.
Then predict the value of area for the following data frame using the tree
command with all defaults

```{r q4newData}
newData <- data.frame(Palmitic=1200, Palmitoleic=120, Stearic=200, Oleic=7000, Linoleic=900, Linolenic=32, Arachidic=60, Eicosenoic=7)
```

What is the resulting prediction? Is the resulting prediction strange? Why or
why not?

```{r}
library(tree)
tree1 <- tree(Area ~ Palmitic + Palmitoleic + Stearic + Oleic + Linoleic + Linolenic + Arachidic + Eicosenoic, data=olive)
summary(tree1)

### Plot tree
plot(tree1)
text(tree1)

predict(tree1, newData)
```

~~2.875. There is no reason why this result is strange.~~  
**2.875. It is strange because Region should be a qualitative variable - but tree is reporting the average value of Region as a numeric variable in the leaf predicted for newdata.**

## Question 5

Load the olive oil data using the commands:

```{r}
library(pgmm)
data(olive)
olive = olive[,-1]
```

Suppose that I fit and prune a tree to get the following diagram. What area
would I predict for a new value of:

```{r}
newData <- data.frame(Palmitic=1200, Palmitoleic=120, Stearic=200, Oleic=7000, Linoleic=900, Linolenic=32, Arachidic=60, Eicosenoic=6)
```

**Area 8**